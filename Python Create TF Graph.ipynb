{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of features (i.e. input units, vocabulary size)\n",
    "features_n = 300\n",
    "#Number of output units (i.e. categories)\n",
    "cats_n = 4\n",
    "#Number of hidden units\n",
    "hidden_units_n = 100\n",
    "\n",
    "#Learning rate, used for testing\n",
    "learning_rate = 0.001\n",
    "#Batch size, used for testing\n",
    "batch_size = 30\n",
    "#Number of epochs,used for testing\n",
    "epochs_n = 10\n",
    "#Reporing frequency, used for testing\n",
    "display_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created successfully\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#\n",
    "# Create TF graph\n",
    "#\n",
    "#\n",
    "with tf.Session() as session:\n",
    "\n",
    "    print(f'Graph created successfully')\n",
    "\n",
    "    #Input\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, features_n], name='inputs')\n",
    "    #Output\n",
    "    outputs = tf.placeholder(tf.float32, shape=[None, cats_n], name='outputs')\n",
    "\n",
    "    #Initialization of weigths to hidden layer\n",
    "    hidden_init = tf.truncated_normal_initializer(stddev=0.01)\n",
    "\n",
    "    #Input->hidden weights\n",
    "    input_to_hidden_w = tf.get_variable(\n",
    "         \"hidden_weights\", dtype=tf.float32, shape=[features_n, hidden_units_n], initializer=hidden_init)\n",
    "\n",
    "    \n",
    "    #Initialization of bias to hidden layer\n",
    "    hidden_bias_init = tf.constant(0., shape=[hidden_units_n], dtype=tf.float32)\n",
    "\n",
    "    #Bias->hidden weights\n",
    "    bias_to_hidden_w = tf.get_variable(\"hidden_bias\", dtype=tf.float32, initializer=hidden_bias_init)\n",
    "\n",
    "    #Hidden layer, activation function is sigmoid\n",
    "    hidden_layer = tf.matmul(inputs, input_to_hidden_w) + bias_to_hidden_w\n",
    "    hidden_layer = tf.nn.sigmoid(hidden_layer)\n",
    "\n",
    "    \n",
    "    #Initialization of weigths to output layer\n",
    "    output_init = tf.truncated_normal_initializer(stddev=0.01)\n",
    "\n",
    "    #Hidden->output weights\n",
    "    hidden_to_output_w = tf.get_variable(\n",
    "         \"output_weights\", dtype=tf.float32, shape=[hidden_units_n, cats_n], initializer=output_init)\n",
    "\n",
    "    #Initialization of bias to output layer\n",
    "    output_bias_init = tf.constant(0., shape=[cats_n], dtype=tf.float32)\n",
    "\n",
    "    #Bias->output weights\n",
    "    bias_to_output_w = tf.get_variable(\"output_bias\", dtype=tf.float32, initializer=output_bias_init)\n",
    "\n",
    "    #Output layer, no activation (i.e. logits)\n",
    "    output_layer = tf.matmul(hidden_layer, hidden_to_output_w) + bias_to_output_w\n",
    "\n",
    "    #Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=outputs, logits=output_layer), name='loss')\n",
    "\n",
    "    #Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(loss)\n",
    "\n",
    "    #Accuracy per trial\n",
    "    correct_prediction = tf.equal(tf.argmax(output_layer, 1), tf.argmax(outputs, 1), name='correct_prediction')\n",
    "\n",
    "    #Overall accuracy/by batch\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "    #Model predictions\n",
    "    cls_prediction = tf.argmax(output_layer, axis=1, name='predictions')\n",
    "\n",
    "    #TF variables initialization\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #Save TF graph\n",
    "    tf.train.Saver()    \n",
    "    tf.train.write_graph(session.graph, \"/home/i/projects/johnsnow/logs\", \"/home/i/projects/johnsnow/mymodel.pb\", False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some aux fuctions from the internet\n",
    "\n",
    "def randomize(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate dummy train/validation set for testing the model\n",
    "#\n",
    "x_train = np.zeros(shape=(batch_size * 1000, features_n))\n",
    "y_train = np.zeros(shape=(batch_size * 1000, cats_n))\n",
    "\n",
    "x_train[:batch_size * 250] = np.random.normal(0, 1, size=(batch_size * 250, features_n))\n",
    "x_train[batch_size * 250:batch_size * 500] = np.random.normal(1, 1, size=(batch_size * 250, features_n))\n",
    "x_train[batch_size * 500:batch_size * 750] = np.random.normal(2, 1, size=(batch_size * 250, features_n))\n",
    "x_train[batch_size * 750:] = np.random.normal(3, 1, size=(batch_size * 250, features_n))\n",
    "\n",
    "y_train[:batch_size * 250, 0] = 1\n",
    "y_train[batch_size * 250:batch_size * 500, 1] = 1\n",
    "y_train[batch_size * 500:batch_size * 750, 2] = 1\n",
    "y_train[batch_size * 750:, 3] = 1\n",
    "\n",
    "x_train, y_train = randomize(x_train, y_train)\n",
    "\n",
    "x_val = x_train[batch_size * 900:,]\n",
    "y_val = y_train[batch_size * 900:,]\n",
    "\n",
    "x_train = x_train[:batch_size * 900,]\n",
    "y_train = y_train[:batch_size * 900,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i/penv/johnsnow/lib/python3.6/site-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Loss=1.35,\tTraining Accuracy=43.3%\n",
      "iter 100:\t Loss=0.71,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.63,\tTraining Accuracy=86.7%\n",
      "iter 300:\t Loss=0.52,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.37,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.39,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.30,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.27,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.24,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 0.25, validation accuracy: 99.3%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Loss=0.26,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.14,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.16,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.12,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.12,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.11,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.09,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.06,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.07,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 0.06, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Loss=0.06,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.05,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.04,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.04,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.03,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.04,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.03,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.03,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 0.02, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 0.01, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 0.00, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 6\n",
      "iter   0:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 6, validation loss: 0.00, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 7\n",
      "iter   0:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 7, validation loss: 0.00, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 8\n",
      "iter   0:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 8, validation loss: 0.00, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 9\n",
      "iter   0:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 9, validation loss: 0.00, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 10\n",
      "iter   0:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 600:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 700:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 800:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 10, validation loss: 0.00, validation accuracy: 100.0%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Testing the model on dummy data\n",
    "#\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "# Number of training iterations in each epoch\n",
    "\n",
    "num_tr_iter = int(len(y_train) / batch_size)\n",
    "\n",
    "for epoch in range(epochs_n):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    # Randomly shuffle the training data at the beginning of each epoch \n",
    "    x_train, y_train = randomize(x_train, y_train)\n",
    "    \n",
    "    for iteration in range(num_tr_iter):\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_batch = {inputs: x_batch, outputs: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % display_freq == 0:\n",
    "            # Calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch = sess.run([loss, accuracy],\n",
    "                                             feed_dict=feed_dict_batch)\n",
    "\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # Run validation after every epoch\n",
    "    feed_dict_valid = {inputs: x_val, outputs: y_val}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'inputs' type=Placeholder>,\n",
       " <tf.Operation 'outputs' type=Placeholder>,\n",
       " <tf.Operation 'hidden_weights/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'hidden_weights/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'hidden_weights/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'hidden_weights/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'hidden_weights/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'hidden_weights/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'hidden_weights' type=VariableV2>,\n",
       " <tf.Operation 'hidden_weights/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_weights/read' type=Identity>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'hidden_bias' type=VariableV2>,\n",
       " <tf.Operation 'hidden_bias/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_bias/read' type=Identity>,\n",
       " <tf.Operation 'MatMul' type=MatMul>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'Sigmoid' type=Sigmoid>,\n",
       " <tf.Operation 'output_weights/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'output_weights/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'output_weights/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'output_weights/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'output_weights/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'output_weights/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'output_weights' type=VariableV2>,\n",
       " <tf.Operation 'output_weights/Assign' type=Assign>,\n",
       " <tf.Operation 'output_weights/read' type=Identity>,\n",
       " <tf.Operation 'Const_1' type=Const>,\n",
       " <tf.Operation 'output_bias' type=VariableV2>,\n",
       " <tf.Operation 'output_bias/Assign' type=Assign>,\n",
       " <tf.Operation 'output_bias/read' type=Identity>,\n",
       " <tf.Operation 'MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'add_1' type=AddV2>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/labels_stop_gradient' type=StopGradient>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Rank' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Shape' type=Shape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Rank_1' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Shape_1' type=Shape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub/y' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub' type=Sub>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice/begin' type=Pack>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice/size' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice' type=Slice>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat/values_0' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat/axis' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat' type=ConcatV2>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Reshape' type=Reshape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Rank_2' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Shape_2' type=Shape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_1/y' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_1' type=Sub>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_1/begin' type=Pack>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_1/size' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_1' type=Slice>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat_1/values_0' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat_1/axis' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat_1' type=ConcatV2>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg' type=SoftmaxCrossEntropyWithLogits>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_2/y' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_2' type=Sub>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_2/begin' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_2/size' type=Pack>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_2' type=Slice>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'Const_2' type=Const>,\n",
       " <tf.Operation 'loss' type=Mean>,\n",
       " <tf.Operation 'gradients/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/grad_ys_0' type=Const>,\n",
       " <tf.Operation 'gradients/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/loss_grad/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'gradients/loss_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/loss_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/loss_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/loss_grad/Shape_2' type=Const>,\n",
       " <tf.Operation 'gradients/loss_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/loss_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/loss_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/loss_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/loss_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/loss_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/loss_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/loss_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/loss_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/zeros_like' type=ZerosLike>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax' type=LogSoftmax>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim' type=Const>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/add_1_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/add_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/add_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/add_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/add_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/add_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/add_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_1_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_1_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/MatMul_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Sigmoid_grad/SigmoidGrad' type=SigmoidGrad>,\n",
       " <tf.Operation 'gradients/add_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/add_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/add_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/add_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/add_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'beta1_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta1_power' type=VariableV2>,\n",
       " <tf.Operation 'beta1_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta1_power/read' type=Identity>,\n",
       " <tf.Operation 'beta2_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta2_power' type=VariableV2>,\n",
       " <tf.Operation 'beta2_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta2_power/read' type=Identity>,\n",
       " <tf.Operation 'hidden_weights/Adam/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'hidden_weights/Adam/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'hidden_weights/Adam/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'hidden_weights/Adam' type=VariableV2>,\n",
       " <tf.Operation 'hidden_weights/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_weights/Adam/read' type=Identity>,\n",
       " <tf.Operation 'hidden_weights/Adam_1/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'hidden_weights/Adam_1/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'hidden_weights/Adam_1/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'hidden_weights/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'hidden_weights/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_weights/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'hidden_bias/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'hidden_bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'hidden_bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'hidden_bias/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'hidden_bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'hidden_bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'output_weights/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'output_weights/Adam' type=VariableV2>,\n",
       " <tf.Operation 'output_weights/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'output_weights/Adam/read' type=Identity>,\n",
       " <tf.Operation 'output_weights/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'output_weights/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'output_weights/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'output_weights/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'output_bias/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'output_bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'output_bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'output_bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'output_bias/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'output_bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'output_bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'output_bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Adam/learning_rate' type=Const>,\n",
       " <tf.Operation 'Adam/beta1' type=Const>,\n",
       " <tf.Operation 'Adam/beta2' type=Const>,\n",
       " <tf.Operation 'Adam/epsilon' type=Const>,\n",
       " <tf.Operation 'Adam/update_hidden_weights/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_hidden_bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_output_weights/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_output_bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/mul' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Adam/mul_1' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign_1' type=Assign>,\n",
       " <tf.Operation 'Adam' type=NoOp>,\n",
       " <tf.Operation 'ArgMax/dimension' type=Const>,\n",
       " <tf.Operation 'ArgMax' type=ArgMax>,\n",
       " <tf.Operation 'ArgMax_1/dimension' type=Const>,\n",
       " <tf.Operation 'ArgMax_1' type=ArgMax>,\n",
       " <tf.Operation 'correct_prediction' type=Equal>,\n",
       " <tf.Operation 'Cast' type=Cast>,\n",
       " <tf.Operation 'Const_3' type=Const>,\n",
       " <tf.Operation 'accuracy' type=Mean>,\n",
       " <tf.Operation 'predictions/dimension' type=Const>,\n",
       " <tf.Operation 'predictions' type=ArgMax>,\n",
       " <tf.Operation 'init' type=NoOp>,\n",
       " <tf.Operation 'save/filename/input' type=Const>,\n",
       " <tf.Operation 'save/filename' type=PlaceholderWithDefault>,\n",
       " <tf.Operation 'save/Const' type=PlaceholderWithDefault>,\n",
       " <tf.Operation 'save/SaveV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/SaveV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/SaveV2' type=SaveV2>,\n",
       " <tf.Operation 'save/control_dependency' type=Identity>,\n",
       " <tf.Operation 'save/RestoreV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign' type=Assign>,\n",
       " <tf.Operation 'save/Assign_1' type=Assign>,\n",
       " <tf.Operation 'save/Assign_2' type=Assign>,\n",
       " <tf.Operation 'save/Assign_3' type=Assign>,\n",
       " <tf.Operation 'save/Assign_4' type=Assign>,\n",
       " <tf.Operation 'save/Assign_5' type=Assign>,\n",
       " <tf.Operation 'save/Assign_6' type=Assign>,\n",
       " <tf.Operation 'save/Assign_7' type=Assign>,\n",
       " <tf.Operation 'save/Assign_8' type=Assign>,\n",
       " <tf.Operation 'save/Assign_9' type=Assign>,\n",
       " <tf.Operation 'save/Assign_10' type=Assign>,\n",
       " <tf.Operation 'save/Assign_11' type=Assign>,\n",
       " <tf.Operation 'save/Assign_12' type=Assign>,\n",
       " <tf.Operation 'save/Assign_13' type=Assign>,\n",
       " <tf.Operation 'save/restore_all' type=NoOp>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's look at the graph\n",
    "session.graph.get_operations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johnsnow",
   "language": "python",
   "name": "johnsnow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
