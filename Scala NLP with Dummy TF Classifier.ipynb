{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages = [\"JohnSnowLabs:spark-nlp:2.4.5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.149:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1588596533031)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//We need a pipeline\n",
    "\n",
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_data: org.apache.spark.sql.DataFrame = [category: string, description: string ... 10 more fields]\n",
       "train_data: org.apache.spark.sql.DataFrame = [category: string, description: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Load processed data\n",
    "val test_data = spark.read.load(\"test.processed.parquet\")\n",
    "val train_data = spark.read.load(\"train.processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml._\n",
       "import org.apache.spark.ml.param.{ParamMap, Params}\n",
       "import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol, HasLabelCol, HasPredictionCol}\n",
       "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\n",
       "import org.apache.spark.ml.linalg.SparseVector\n",
       "import org.tensorflow.Graph\n",
       "import com.johnsnowlabs.ml.tensorflow._\n",
       "import com.johnsnowlabs.nlp.util.io.ResourceHelper\n",
       "import org.apache.commons.io.IOUtils\n",
       "import scala.util.Random\n",
       "defined class DummyTFClassifier\n",
       "defined trait DummyParams\n",
       "defined trait DummyDataProcessor\n",
       "defined class DummyModel\n",
       "defined class DummyEstimator\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//This is a very very ugly way to setup a set of classes but it is useful and faster\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.param.{ParamMap, Params}\n",
    "import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol, HasLabelCol, HasPredictionCol}\n",
    "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\n",
    "import org.apache.spark.ml.linalg.SparseVector\n",
    "import org.tensorflow.Graph\n",
    "import com.johnsnowlabs.ml.tensorflow._\n",
    "import com.johnsnowlabs.nlp.util.io.ResourceHelper\n",
    "import org.apache.commons.io.IOUtils\n",
    "import scala.util.Random\n",
    "\n",
    "/*\n",
    "    A Scala wrapper around a TF graph. Loads the graph from a .pb file and provides\n",
    "    basic functionality for working with the model, i.e. training and testing.\n",
    "    All parameters are hard-coded except for batch size and number of epochs\n",
    "    Inspired by the SparkNLP source, but simplified\n",
    "*/\n",
    "class DummyTFClassifier(var tfGraphFileName: String) {\n",
    "    private val inputKey = \"inputs\"\n",
    "    private val labelKey = \"outputs\"\n",
    "    private val learningRateKey = \"Adam/learning_rate\"\n",
    "    private val trainingKey = \"Adam\"\n",
    "\n",
    "    private val predictionKey = \"predictions\"\n",
    "    private val lossKey = \"loss\"\n",
    "    private val accuracyKey = \"accuracy\"\n",
    "    private val initKey = \"init\"    \n",
    "    \n",
    "    lazy private val tf_model = {\n",
    "        \n",
    "        val graph = new Graph()\n",
    "        val graphStream = ResourceHelper.getResourceStream(tfGraphFileName)\n",
    "        val graphBytesDef = IOUtils.toByteArray(graphStream)\n",
    "\n",
    "        graph.importGraphDef(graphBytesDef)\n",
    "        \n",
    "        new TensorflowWrapper(\n",
    "            Variables(Array.empty[Byte], Array.empty[Byte]), \n",
    "            graph.toGraphDef\n",
    "        )\n",
    "    }\n",
    "    \n",
    "     def train(features: Array[Array[Float]], labels: Array[Array[Float]], endEpoch: Int = 100, batchSize : Int = 10) = {\n",
    "         \n",
    "        \n",
    "        val zippedDataset = features.zip(labels).toSeq\n",
    "        val trainingDataset = Random.shuffle(zippedDataset)\n",
    "         \n",
    "        tf_model.createSession().runner.addTarget(initKey).run()\n",
    "        \n",
    "        for( epoch <- 1 to endEpoch){\n",
    "            \n",
    "            var loss = 0.0f\n",
    "            var acc = 0.0f            \n",
    "            var batches = 0\n",
    "            \n",
    "            for (batch <- trainingDataset.toArray.grouped(batchSize)){\n",
    "                \n",
    "                val tensors = new TensorResources()\n",
    "                \n",
    "                val featuresArray = batch.map(x => x._1)\n",
    "                val labelsArray = batch.map(x => x._2)\n",
    "                \n",
    "                val inputTensor = tensors.createTensor(featuresArray)\n",
    "                val labelTensor = tensors.createTensor(labelsArray)        \n",
    "\n",
    "                val calculated = tf_model\n",
    "                          .getSession()\n",
    "                          .runner\n",
    "                          .feed(inputKey, inputTensor)\n",
    "                          .feed(labelKey, labelTensor)\n",
    "                          .addTarget(trainingKey)\n",
    "                          .fetch(lossKey)\n",
    "                          .fetch(accuracyKey)\n",
    "                           .fetch(predictionKey)\n",
    "                          .run();\n",
    "                loss += TensorResources.extractFloats(calculated.get(0))(0);\n",
    "                acc += TensorResources.extractFloats(calculated.get(1))(0);\n",
    "                batches += 1\n",
    "                tensors.clearTensors()   \n",
    "                \n",
    "            }\n",
    "                        \n",
    "            println(f\"Epoch ${epoch}/$endEpoch loss: ${loss/batches} - accuracy: ${acc/batches}\")\n",
    "\n",
    "        }  \n",
    "    }\n",
    "    \n",
    "    def predict(features: Array[Array[Float]], labels: Array[Array[Float]]) = {\n",
    "        \n",
    "        val tensors = new TensorResources()\n",
    "        \n",
    "        val inputTensor = tensors.createTensor(features)\n",
    "        val labelTensor = tensors.createTensor(labels)        \n",
    "\n",
    "        val calculated = tf_model\n",
    "                  .getSession()\n",
    "                  .runner\n",
    "                  .feed(inputKey, inputTensor)\n",
    "                  .feed(labelKey, labelTensor)\n",
    "                  .fetch(predictionKey)\n",
    "                  .fetch(accuracyKey)\n",
    "                  .run();\n",
    "        \n",
    "        val result = TensorResources.extractLongs(calculated.get(0))\n",
    "        var acc = TensorResources.extractFloats(calculated.get(1))(0)\n",
    "        tensors.clearTensors()          \n",
    "        \n",
    "        println(f\"Prediction accuracy is ${acc}\")\n",
    "\n",
    "        result\n",
    "    }\n",
    "}\n",
    "\n",
    "trait DummyParams extends Params\n",
    "  with HasInputCol with HasLabelCol with HasPredictionCol{\n",
    "  \n",
    "}\n",
    "\n",
    "/*\n",
    "    A simple and very inefficient way of annotating a Spark dataframe\n",
    "    In order to make training easier, I've limitted the number of rows used for trainig to TRAIN_SET_SIZE\n",
    "*/\n",
    "trait DummyDataProcessor {\n",
    "    \n",
    "    private val TRAIN_SET_SIZE = 100000\n",
    "    \n",
    "    def process(data: Dataset[_], labelCol: String, inputCol: String): (Array[Array[Float]], Array[Array[Float]], DataFrame) = {\n",
    "       //retreve labels\n",
    "        val unique_labels = data.select(labelCol).distinct().collect().toArray.map(x => x(0).toString)\n",
    "        //get the index of a label\n",
    "        val label_to_index = udf {(label: String) => \n",
    "          unique_labels.indexOf(label)\n",
    "        }\n",
    "\n",
    "        val label_id = labelCol + \"_id\"\n",
    "\n",
    "        val ndata = data\n",
    "            .limit(TRAIN_SET_SIZE)\n",
    "            .withColumn(label_id, label_to_index(col(labelCol)))\n",
    "            .withColumn(\"mon_id\", monotonically_increasing_id())              \n",
    "        \n",
    "        val labels = ndata.select(label_id).collect().map(r => r.getAs[Int](label_id)).map(\n",
    "            i => {\n",
    "                val v = new SparseVector(unique_labels.length, Array(i), Array(1)).toArray\n",
    "                v.map(x => x.toFloat)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        val features = ndata.select(inputCol).collect().map {\n",
    "             row => row.get(0).asInstanceOf[SparseVector].toDense.toArray.map(d => d.toFloat)\n",
    "        }        \n",
    "        \n",
    "        return (features, labels, ndata)\n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    "    Spark model for transforming the data to be classified\n",
    "*/\n",
    "class DummyModel (override val uid: String, tfdModel: DummyTFClassifier) \n",
    "    extends Model[DummyModel] with DummyParams with DummyDataProcessor {\n",
    "              \n",
    "    override def copy(extra: ParamMap): DummyModel = defaultCopy(extra)\n",
    "\n",
    "    override def transform(dataset: Dataset[_]): DataFrame = {\n",
    "        val (features, labels, ndata) = this.process(dataset, $(labelCol), $(inputCol))\n",
    "\n",
    "\n",
    "        val prediction_labels = ndata.select($(labelCol)).distinct().collect().toArray.map(x => x(0).toString)\n",
    "        \n",
    "        val predictions = tfdModel.predict(features, labels)\n",
    "        \n",
    "        val get_prediction = udf {(mon_id: Integer) => \n",
    "          prediction_labels(predictions(mon_id).toInt)\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        val t_ndata = ndata.withColumn($(predictionCol), get_prediction(col(\"mon_id\")))\n",
    "        \n",
    "        t_ndata\n",
    "    }\n",
    " \n",
    "    override def transformSchema(schema: StructType): StructType = {\n",
    "        StructType(Seq(StructField($(predictionCol), StringType, true)).++(schema))\n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    "    Spark estimator for training the dummy TF model\n",
    "*/\n",
    "class DummyEstimator(override val uid: String) extends Estimator[DummyModel]\n",
    "  with DummyParams with DefaultParamsWritable with DummyDataProcessor{\n",
    "\n",
    "  def this() = this(Identifiable.randomUID(\"DummyEstimator\"))\n",
    "\n",
    "  def setInputCol(value: String): this.type = set(inputCol -> value)    \n",
    "  def setLabelCol(value: String): this.type = set(labelCol -> value)    \n",
    "  def setPredictionCol(value: String): this.type = set(predictionCol -> value)    \n",
    "  \n",
    "  override def transformSchema(schema: StructType): StructType = {\n",
    "    schema\n",
    "  }\n",
    "\n",
    "  override def copy(extra: ParamMap): Estimator[DummyModel] = defaultCopy(extra)\n",
    "\n",
    "\n",
    "  override def fit(dataset: Dataset[_]): DummyModel = {\n",
    "            \n",
    "     val (features, labels, ndata) = this.process(dataset, $(labelCol), $(inputCol))\n",
    "\n",
    "    val myDummy = new DummyTFClassifier(\"/home/i/projects/johnsnow/mymodel.pb\")\n",
    "    \n",
    "    myDummy.train(features, labels, 100, 100)\n",
    "    //do some fitting here\n",
    "      \n",
    "    copyValues(new DummyModel(uid, myDummy).setParent(this))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dummy: DummyEstimator = DummyEstimator_7a2d79103e3e\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_0d08671908dd\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Compose a pipeline\n",
    "val dummy = new DummyEstimator().setLabelCol(\"category\").setInputCol(\"features\").setPredictionCol(\"prediction\")\n",
    "val pipeline = new Pipeline().setStages(Array(dummy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 loss: 0.65244454 - accuracy: 0.7649796\n",
      "Epoch 2/100 loss: 0.5553829 - accuracy: 0.7923904\n",
      "Epoch 3/100 loss: 0.5466695 - accuracy: 0.79494053\n",
      "Epoch 4/100 loss: 0.54079574 - accuracy: 0.79668015\n",
      "Epoch 5/100 loss: 0.5359634 - accuracy: 0.7979803\n",
      "Epoch 6/100 loss: 0.53156924 - accuracy: 0.79931015\n",
      "Epoch 7/100 loss: 0.52734375 - accuracy: 0.8006404\n",
      "Epoch 8/100 loss: 0.52313375 - accuracy: 0.80223006\n",
      "Epoch 9/100 loss: 0.51882946 - accuracy: 0.80361944\n",
      "Epoch 10/100 loss: 0.5143486 - accuracy: 0.8050593\n",
      "Epoch 11/100 loss: 0.5096366 - accuracy: 0.8068894\n",
      "Epoch 12/100 loss: 0.50465864 - accuracy: 0.8088391\n",
      "Epoch 13/100 loss: 0.49939582 - accuracy: 0.8108293\n",
      "Epoch 14/100 loss: 0.4938512 - accuracy: 0.81299967\n",
      "Epoch 15/100 loss: 0.4880399 - accuracy: 0.81526995\n",
      "Epoch 16/100 loss: 0.48197564 - accuracy: 0.8174898\n",
      "Epoch 17/100 loss: 0.4756737 - accuracy: 0.8199401\n",
      "Epoch 18/100 loss: 0.46914673 - accuracy: 0.8225399\n",
      "Epoch 19/100 loss: 0.46241516 - accuracy: 0.82542014\n",
      "Epoch 20/100 loss: 0.45551005 - accuracy: 0.8286303\n",
      "Epoch 21/100 loss: 0.44846895 - accuracy: 0.8312796\n",
      "Epoch 22/100 loss: 0.44133848 - accuracy: 0.83374995\n",
      "Epoch 23/100 loss: 0.43416652 - accuracy: 0.83667004\n",
      "Epoch 24/100 loss: 0.426998 - accuracy: 0.8398799\n",
      "Epoch 25/100 loss: 0.41987553 - accuracy: 0.84255975\n",
      "Epoch 26/100 loss: 0.41284403 - accuracy: 0.8448997\n",
      "Epoch 27/100 loss: 0.40592584 - accuracy: 0.84782976\n",
      "Epoch 28/100 loss: 0.39913824 - accuracy: 0.8503897\n",
      "Epoch 29/100 loss: 0.3924898 - accuracy: 0.85312957\n",
      "Epoch 30/100 loss: 0.38598347 - accuracy: 0.8556198\n",
      "Epoch 31/100 loss: 0.3796225 - accuracy: 0.85817945\n",
      "Epoch 32/100 loss: 0.3734157 - accuracy: 0.8608194\n",
      "Epoch 33/100 loss: 0.36736694 - accuracy: 0.8636489\n",
      "Epoch 34/100 loss: 0.361476 - accuracy: 0.8658592\n",
      "Epoch 35/100 loss: 0.35574004 - accuracy: 0.86804944\n",
      "Epoch 36/100 loss: 0.3501551 - accuracy: 0.8702995\n",
      "Epoch 37/100 loss: 0.3447189 - accuracy: 0.87273943\n",
      "Epoch 38/100 loss: 0.33943024 - accuracy: 0.8747896\n",
      "Epoch 39/100 loss: 0.33428958 - accuracy: 0.87710935\n",
      "Epoch 40/100 loss: 0.32929277 - accuracy: 0.87927926\n",
      "Epoch 41/100 loss: 0.3244356 - accuracy: 0.8816495\n",
      "Epoch 42/100 loss: 0.31971508 - accuracy: 0.88322973\n",
      "Epoch 43/100 loss: 0.31512538 - accuracy: 0.8852698\n",
      "Epoch 44/100 loss: 0.31066188 - accuracy: 0.8873898\n",
      "Epoch 45/100 loss: 0.30632654 - accuracy: 0.8893301\n",
      "Epoch 46/100 loss: 0.30211326 - accuracy: 0.8910097\n",
      "Epoch 47/100 loss: 0.29801145 - accuracy: 0.8923998\n",
      "Epoch 48/100 loss: 0.2940166 - accuracy: 0.89423025\n",
      "Epoch 49/100 loss: 0.29012662 - accuracy: 0.89616\n",
      "Epoch 50/100 loss: 0.28633833 - accuracy: 0.8976601\n",
      "Epoch 51/100 loss: 0.28264865 - accuracy: 0.8992599\n",
      "Epoch 52/100 loss: 0.27905414 - accuracy: 0.90085\n",
      "Epoch 53/100 loss: 0.2755499 - accuracy: 0.9022197\n",
      "Epoch 54/100 loss: 0.27213112 - accuracy: 0.9037494\n",
      "Epoch 55/100 loss: 0.26879573 - accuracy: 0.90482914\n",
      "Epoch 56/100 loss: 0.26554537 - accuracy: 0.9062794\n",
      "Epoch 57/100 loss: 0.26238027 - accuracy: 0.90755916\n",
      "Epoch 58/100 loss: 0.25929567 - accuracy: 0.90887916\n",
      "Epoch 59/100 loss: 0.25628862 - accuracy: 0.91024876\n",
      "Epoch 60/100 loss: 0.25335434 - accuracy: 0.9115791\n",
      "Epoch 61/100 loss: 0.25048825 - accuracy: 0.9128392\n",
      "Epoch 62/100 loss: 0.24768731 - accuracy: 0.9138992\n",
      "Epoch 63/100 loss: 0.24494801 - accuracy: 0.9151292\n",
      "Epoch 64/100 loss: 0.24226819 - accuracy: 0.91623914\n",
      "Epoch 65/100 loss: 0.23964538 - accuracy: 0.9171489\n",
      "Epoch 66/100 loss: 0.23707645 - accuracy: 0.9180088\n",
      "Epoch 67/100 loss: 0.23455921 - accuracy: 0.91895896\n",
      "Epoch 68/100 loss: 0.23209229 - accuracy: 0.9202294\n",
      "Epoch 69/100 loss: 0.22967443 - accuracy: 0.9213496\n",
      "Epoch 70/100 loss: 0.22730403 - accuracy: 0.9222698\n",
      "Epoch 71/100 loss: 0.22497801 - accuracy: 0.9231997\n",
      "Epoch 72/100 loss: 0.22270028 - accuracy: 0.92425966\n",
      "Epoch 73/100 loss: 0.22047535 - accuracy: 0.9252493\n",
      "Epoch 74/100 loss: 0.21829817 - accuracy: 0.92612904\n",
      "Epoch 75/100 loss: 0.21616521 - accuracy: 0.9268891\n",
      "Epoch 76/100 loss: 0.21407476 - accuracy: 0.9277588\n",
      "Epoch 77/100 loss: 0.21202469 - accuracy: 0.928509\n",
      "Epoch 78/100 loss: 0.21001242 - accuracy: 0.9292892\n",
      "Epoch 79/100 loss: 0.20803426 - accuracy: 0.93018925\n",
      "Epoch 80/100 loss: 0.20608824 - accuracy: 0.93099934\n",
      "Epoch 81/100 loss: 0.20418406 - accuracy: 0.9316796\n",
      "Epoch 82/100 loss: 0.20231892 - accuracy: 0.9325897\n",
      "Epoch 83/100 loss: 0.20048735 - accuracy: 0.9333598\n",
      "Epoch 84/100 loss: 0.19868763 - accuracy: 0.93412995\n",
      "Epoch 85/100 loss: 0.19691816 - accuracy: 0.9347697\n",
      "Epoch 86/100 loss: 0.19517855 - accuracy: 0.93537\n",
      "Epoch 87/100 loss: 0.1934673 - accuracy: 0.9361198\n",
      "Epoch 88/100 loss: 0.19178425 - accuracy: 0.9365799\n",
      "Epoch 89/100 loss: 0.19012837 - accuracy: 0.93737996\n",
      "Epoch 90/100 loss: 0.18849818 - accuracy: 0.9378701\n",
      "Epoch 91/100 loss: 0.18689264 - accuracy: 0.9383701\n",
      "Epoch 92/100 loss: 0.18531017 - accuracy: 0.9387801\n",
      "Epoch 93/100 loss: 0.18374996 - accuracy: 0.9392003\n",
      "Epoch 94/100 loss: 0.18221036 - accuracy: 0.9399004\n",
      "Epoch 95/100 loss: 0.18069066 - accuracy: 0.94056034\n",
      "Epoch 96/100 loss: 0.17918958 - accuracy: 0.9410105\n",
      "Epoch 97/100 loss: 0.1777068 - accuracy: 0.9416605\n",
      "Epoch 98/100 loss: 0.17624104 - accuracy: 0.9421505\n",
      "Epoch 99/100 loss: 0.17479241 - accuracy: 0.9427004\n",
      "Epoch 100/100 loss: 0.17336076 - accuracy: 0.94343036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "m1: org.apache.spark.ml.PipelineModel = pipeline_0d08671908dd\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Fit, i.e. tain the dummy TF model\n",
    "//Mind that we are current using TRAIN_SET_SIZE rows to save computational resource (my laptop is going to explode of this buggy Java)\n",
    "val m1 = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 0.7501316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [category: string, description: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//And transform the test_data\n",
    "val results = m1.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|category|prediction|\n",
      "+--------+----------+\n",
      "|Business|     World|\n",
      "|  Sports|    Sports|\n",
      "|  Sports|    Sports|\n",
      "|   World|  Business|\n",
      "|Business|  Business|\n",
      "|Business|     World|\n",
      "|Sci/Tech|  Sci/Tech|\n",
      "|Sci/Tech|  Sci/Tech|\n",
      "|Sci/Tech|  Sci/Tech|\n",
      "|   World|  Business|\n",
      "|Sci/Tech|  Sci/Tech|\n",
      "|  Sports|    Sports|\n",
      "|  Sports|  Business|\n",
      "|  Sports|    Sports|\n",
      "|Business|     World|\n",
      "|Business|     World|\n",
      "|  Sports|    Sports|\n",
      "|Business|  Sci/Tech|\n",
      "|  Sports|    Sports|\n",
      "|  Sports|    Sports|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Just checking\n",
    "results.select(\"category\", \"prediction\").orderBy(rand()).show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
