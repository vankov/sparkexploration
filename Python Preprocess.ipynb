{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Start SparkNLP\n",
    "#\n",
    "from sparknlp.annotator import * \n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline \n",
    "from pyspark.ml import Pipeline\n",
    "import sparknlp  \n",
    "import nltk\n",
    "from pyspark.sql.functions import *\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import IDF, CountVectorizer, HashingTF\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of features(vocabulary size)\n",
    "VOCAB_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-04 14:46:45--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24032125 (23M) [text/plain]\n",
      "Saving to: ‘news_category_train.csv.1’\n",
      "\n",
      "news_category_train 100%[===================>]  22,92M  1,75MB/s    in 14s     \n",
      "\n",
      "2020-05-04 14:47:00 (1,60 MB/s) - ‘news_category_train.csv.1’ saved [24032125/24032125]\n",
      "\n",
      "--2020-05-04 14:47:00--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1504408 (1,4M) [text/plain]\n",
      "Saving to: ‘news_category_test.csv.2’\n",
      "\n",
      "news_category_test. 100%[===================>]   1,43M  1,08MB/s    in 1,3s    \n",
      "\n",
      "2020-05-04 14:47:02 (1,08 MB/s) - ‘news_category_test.csv.2’ saved [1504408/1504408]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#download train and test data\n",
    "#data is taken from https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32\n",
    "\n",
    "! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
    "! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into spark\n",
    "train_data = spark.read.option(\"header\", \"true\").csv(\"news_category_train.csv\") \n",
    "test_data = spark.read.option(\"header\", \"true\").csv(\"news_category_test.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get English stop words from nltk\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#Define the pipeline components\n",
    "#\n",
    "#We generate features by tokenizing single words and bigrams, getting their lemmas, removing stopwords\n",
    "#selecting the most frequently occuring words (at at least 10 documents) and weighting them by their TF-IDF\n",
    "#\n",
    "#The final annotation is in the 'features' column, which is going to be sued to train the Dummy TF model\\\n",
    "#It is a sparse vector of size VOCAB_SIZE which constitute the input to the model\n",
    "doccumentAssembler = DocumentAssembler().setInputCol(\"description\").setOutputCol(\"document\").setCleanupMode(\"shrink\")\n",
    "sentenceDetector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentences\")\n",
    "tokenizer = Tokenizer().setInputCols([\"sentences\"]).setOutputCol(\"token\")\n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normal\").setLowercase(True)\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"normal\"]).setOutputCol(\"lemma\")\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"lemma\"]).setOutputCol(\"clean_lemma\").setCaseSensitive(False).setStopWords(eng_stopwords)\n",
    "ngramGenerator = NGramGenerator() \\\n",
    "            .setInputCols([\"clean_lemma\"]) \\\n",
    "            .setOutputCol(\"ngrams\") \\\n",
    "            .setN(2) \\\n",
    "            .setEnableCumulative(True) \\\n",
    "            .setDelimiter(\"_\")\n",
    "ngramsFinisher = Finisher().setInputCols([\"ngrams\"]).setCleanAnnotations(False)\n",
    "vectorizer = CountVectorizer(inputCol=\"finished_ngrams\", outputCol=\"rawFeatures\", vocabSize=VOCAB_SIZE, minDF=10.0)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build pipline\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "    doccumentAssembler, sentenceDetector, tokenizer, normalizer, lemmatizer, stopwords_cleaner, \n",
    "    ngramGenerator, ngramsFinisher, vectorizer, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform. There is no rocket science here\n",
    "\n",
    "m = nlpPipeline.fit(train_data)\n",
    "p_train_data = m.transform(train_data)\n",
    "p_test_data = m.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the processed data so that we can continue with Scala\n",
    "p_train_data.write.mode(\"overwrite\").save(\"train.processed.parquet\")\n",
    "p_test_data.write.mode(\"overwrite\").save(\"test.processed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('p3': venv)",
   "language": "python",
   "name": "python36864bitp3venv38f3a5947b564cb69cee0d670d158b95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
